---
title: "1. Finance Projekt WS21/22"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    self_contained: true
#output: rmdformats::downcute
#output: rmdformats::robobook
#output: rmdformats::material
#output: rmdformats::readthedown
#https://www.rdocumentation.org/packages/rmarkdown/versions/2.11/topics/html_document
# output:
#   rmdformats::downcute:
#     code_folding: show
#     code_download: true
#     self_contained: true
#     thumbnails: false
#     lightbox: true
#output: pdf_document
editor_options: 
  chunk_output_type: console
header-includes:
  \usepackage{fvextra}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
#source_python("KI_Abgabe_1_v1.py")
# library(knitr)
# opts_chunk$set(tidy.opts=list(width.cutoff=20),tidy=TRUE)
```

```{css, echo=FALSE}
.Wrap {
  max-width: 1120px;
}
.main-content {
  max-width: 1120px;
}
<!-- .row { -->
<!--     height: 0px; -->
<!--     position: relative; -->
<!--     top: 25px; -->
<!--     z-index: 999999; -->
<!--     right: 3px; -->
<!-- } -->

.pull-right {
    position: relative !important;
    width: 0 !important;
    padding: 0px !important;
    margin: 0px !important;
    left: 5px !important;
}
.collapse.in {
  top: -20px !important;
  position: relative !important;
}
.row {
height: 0px !important;
}
```
*Version:* V1.3  
*Last change:* 31.10.2021  
*Author:* Axel Roth, Aysegül Dursun  

# Einleitung
Das Ziel dieses Projektes ist es ein [einfaches Perzeptron](https://de.wikipedia.org/wiki/Perzeptron) mit grundlegenden Python-Befehlen zu programmieren. Ein Perzepton gehört zu den einfachsten neuronalen Netzen, da sie nur aus einem einzelnen künstlichen Neuron bestehen. Für unser Perzeptron werden folgende Eigenschaften bzw. Parameter festgelegt, auf die erst später genauer eingegangen wird:  
Epochen bzw. Iterationen der Trainingsphase := `iterations`   
Der Gewichtsvektor für den ersten Durchlauf := $w_0$  
Die Lernrate beim neujustieren der Gewichte := $\alpha =$ `alpha`    
Bias-Wert (verschiebung der Aktivierungsschwelle) := $\beta :=$ `biasVal`     
eine Aktivierungsfunktion:  
$$ 
step_0(s)= 
\begin{cases}
    1,& s	\geq 0\\
    0,& s < 0
\end{cases}
$$
bzw.
```{python}
def step(s):
    if( s >= 0 ):
        return(1)
    else:
        return(0)
```
und die Trainingsdaten mit $x_i,_j$ als Input-Werte und $y_i$ als korrekte Antwort bzw. Output des Szenarios $i$:  
$$
\left[
\begin{array}{ccc|c}
x_i,_0 & x_i,_1 & x_i,_2 & y_i \\
\end{array}
\right]
$$
$$
\left[
\begin{array}{ccc|c}
1 & 0 & 0 & 0 \\
1 & 0 & 1 & 1 \\
1 & 1 & 0 & 1 \\
1 & 1 & 1 & 1 \\
\end{array}
\right]
$$
Der Wert von $x_i,_1$ ist immer 1 und gehört per se nicht wirklich zu den Input Werten. Trotzdem wird er in den Trainingsdaten aufgenommen, um die Aktivierungsfunktion zu vereinfachen und um die Aktivierungsschwelle `biasVal` mitlernen zu lassen.  
Zusätzlich werden wir folgende Packages und Optionen verwenden:
```{python}
import numpy as np
import random as ra
import pandas as pd
import matplotlib.pyplot as pyplot
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
```

# Beschreibung des Ablaufes
Unser Perzeptron soll aus den Trainingsdaten ein zufälliges Szenario auswählen und die sogenannte `forward()` Funktion auswerten, um einen Schätzwert für den Output des Szenarios zu erhalten. Anschließend werden durch die `backward()` Funktion die verwendeten Gewichte, bedingt auf die Güte der Antwort, neujustiert. Im Anschluss werden die Ergebnisse visualisiert und analysiert.


# Funktionen des Perzeptrons
Die `forward()` Funktion ist lediglich die Auswertung der $step_0$ Funktion mit der gewichteten Summe der Input-Werte $x_i,_j$ eines Szenarios $i$ und den Gewichten $w_k$. Da die gewichtete Summe Äquivalent zum Skalarprodukt ist, wird dieses hier verwendet:
```{python}
def forward(w, x):
    return( step( np.dot(w.transpose(), x) ) )
```
  
Die `backward()` Funktion muss nun den Output bewerten und die Gewichte dementsprechend neujustieren. Dies geschieht über folgende Logik:
$$w_i(t+1) = w_i(t) + \Delta w_i(t)$$ mit $$\Delta w_i(t) = \alpha*(y_i-\hat{y}_i(t))*x_i,_j$$
Hier kann beobachtet werden, das die Lernrate $\alpha$ gerade die Intensität der Neujustierung ist und das bei richtigem Output $\hat{y}$ keine Änderung an den Gewichten vorgenommen wird. Im Code sieht diese Funktion wie folgt aus:
```{python}
def backward(w, x, correct_answer, approx_answer, alpha):
    return(w + alpha * (correct_answer - approx_answer) * x)
```

In der Funktion `fit()` lernt das Perzeptron basierend auf den Input-Werten einen immer besseren Output zu generieren, indem es in jeder Iteration ein zufälliges Szenario auswählt und darauf die `forward()` und `backward()` Funktion anwendet:
```{python}
def fit(alpha, iterations, training_data_set, wgts):
    
    
    df = pd.DataFrame(
      columns=["iteration", "input_wgts", "approx_answer", "correct_answer", "error", "new_wgts"])
    
    
    for i in range(1, iterations+1):
        random_index = ra.randint(0, len(training_data_set)-1) 
        input_wgts = wgts
        input_x = training_data_set[random_index][0]
        correct_answer = training_data_set[random_index][1]

        approx_answer = forward(w = input_wgts, x = input_x)
        wgts = backward(w = input_wgts, 
                           x = input_x,
                           correct_answer = correct_answer,
                           approx_answer = approx_answer,
                           alpha = alpha)
        df = df.append({'iteration': i, 
                        'input_wgts': input_wgts, 
                        'approx_answer': approx_answer, 
                        'correct_answer': correct_answer, 
                        'error': correct_answer - approx_answer, 
                        'new_wgts': wgts}, 
                       ignore_index=True)
    return(df)
```
Zusätzlich werden die Informationen innerhalb der Iterationen in einem Dataframe `df` aus dem Package pandas gespeichert, da dieser Spaltennamen unterstützt.

# Trainingsphase des Perzeptron
Beispielhaft werden für die Trainingsphase 30 Iterationen verwendet und die Aktivierungsschwelle und Lernrate auf 1 gesetzt. Für die Anfangsgewichte wird ein Vektor aus Nullen erstellt, in dem die erste Null durch die negative Aktivierungsschwelle `-biasVal` ersetzt wird. Hierbei wird ersichtlich, weshalb es Aktivierungsschwelle heißt, da nun die gewichtete Summe gerade dann positiv ist, wenn die wirklichen Input-Werte $x_i,_j$ mit $j>0$ gerade die Aktivierungsschwelle überschreiten, wodurch die $step_0(s)$ Funktion dann den Wert 1 annimmt.
```{python}
alpha = 1
iterations = 30
biasVal = 1
training_data_set = [(np.array([1,0,0]), 0),
                     (np.array([1,0,1]), 1),
                     (np.array([1,1,0]), 1),
                     (np.array([1,1,1]), 1)]

wgts = np.zeros(len(training_data_set[0][0]))
wgts[0] = -biasVal


result_fit = fit(alpha = alpha, iterations = iterations, training_data_set = training_data_set, wgts = wgts)
print("Result of Fitting: \n", result_fit, "\n\n")
```
Im Ergebnis `result_fit` kann man die funktionsweise der `backward()` Funktion erkennen, da die Input Gewichte gerade dann neujustiert werden, wenn der Fehler ungleich Null ist.

Die Gewichte der letzten Iteration sind folgende:
```{python}
optimized_wgts = result_fit["new_wgts"][result_fit.last_valid_index()]
print("Optimized Wgts are: ", optimized_wgts, "\n")
```

# Testphase des trainierten Perzeptron
Hierfür wird eine Funktion `test()` erstellt, die eine vereinfachte Version der `fit()` Funktion ist, in der nur `forward()` einmalig über jedes Szenario ausgeführt wird.
```{python}
def test(wgts, test_data_set):

    df = pd.DataFrame(columns=["iteration", "inputs_x", "approx_answer", "correct_answer", "error"])


    for i in range(0, len(test_data_set)):

        x = test_data_set[i][0]
        correct_answer = test_data_set[i][1]

        approx_answer = forward(w = wgts, x = x)

        df = df.append({'iteration': i,
                        "inputs_x": x,
                        'approx_answer': approx_answer,
                        'correct_answer': correct_answer,
                        'error': correct_answer - approx_answer},
                       ignore_index=True)
    return(df)

```
Theoretisch sollte man im idealfall das Testen nicht auf den Trainingsdaten machen, dies ist in unserem Fall aber nicht möglich, da wir nur diese zur verfügung haben.
```{python}
result_test = test(wgts = optimized_wgts, test_data_set = training_data_set)   
print("Result of Test: \n", result_test, "\n\n")
```
Hier ist zusehen, das kein Fehler mehr entstanden ist und somit entscheidet sich das Perzeptron immer für den richtigen Output, bezogen auf die Trainingsdaten.


# Visualisierung der Ergebnisse
Nun ein paar Charts, um einen besseren Einblick in das Training des Perzeptrons zu bekommen.  

#### Fehler innerhalb der Trainingsphase und der Testphase
```{python}
fig, axs = pyplot.subplots(2)
axs[0].plot(result_fit["iteration"], result_fit["error"], "g", linewidth=2)
axs[0].set_title("Training")
axs[0].set_ylabel("Error")
axs[0].set_xlabel("Iteration")
axs[1].plot(result_test["iteration"], result_test["error"], "g", linewidth=2)
axs[1].set_title("Test")
axs[1].set_ylabel("Error")
axs[1].set_xlabel("Test Row")
axs[1].set_xticks(range(len(training_data_set)))
pyplot.tight_layout()
pyplot.show()
```

#### Klassifizierung der Input-Werte
In dem nachfolgenden Charts wird erkenntlich, wann es dem Perzeptron gelingt sich soweit zu optimieren, das es immer den korrekten Output liefert. Die Punkte visualisieren die Trainingsdaten, wobei die schwarzen Punkte für den Output 0 und die roten für den Output 1 stehen. Genau dann wenn diese Punkte liniear trennbar sind, kann das Perzeptron optimal auf die Trainingsdaten gefittet werden.  
Die Gerade wird über die Gewichte mit folgender Formel definiert:
$$f(x) = - \frac{w_0}{w_2} - \frac{w_1}{w_2} \cdot x $$
```{python}
data_trennlinie = fit(
  alpha = alpha, 
  iterations = iterations, 
  training_data_set = training_data_set, 
  wgts = np.array([-0.28, 0.02, 0.05]))   

pic_rows = 10
pic_cols = 3
fig, axs = pyplot.subplots(pic_rows, pic_cols, figsize=(9,35))

for k in range(pic_rows):
    for i in range(pic_cols):
         w = data_trennlinie["input_wgts"][k*pic_cols+i]
         x1 = np.arange(-0.5, 1.5, 0.01)
         x2 = np.zeros(len(np.arange(-0.5, 1.5, 0.01)))
         if w[2] != 0:
             x2 = - w[0] / w[2] - w[1] / w[2] * x1
         
         axs[k, i].plot(x1, x2, "g", linewidth=2)
         axs[k, i].set_ylim(-2, 6)
         axs[k, i].plot(0,0,"ko")
         axs[k, i].plot([0,1,1],[1,0,1],"ro", )
         axs[k, i].set_title("Schritt " + str(k*pic_cols+i))
pyplot.tight_layout()
pyplot.show()
```

# Anhang
Der komplette Quellcode:
```{python, eval=FALSE}
import numpy as np
import random as ra
import pandas as pd
import matplotlib.pyplot as pyplot
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)


def step(s):
    if( s >= 0 ):
        return(1)
    else:
        return(0)

def forward(w, x):
    return( step( np.dot(w.transpose(), x) ) )

def backward(w, x, correct_answer, approx_answer, alpha):
    return(w + alpha * (correct_answer - approx_answer) * x)


def fit(alpha, iterations, training_data_set, wgts):
    
    
    df = pd.DataFrame(columns=["iteration", "input_wgts", "approx_answer", "correct_answer", "error", "new_wgts"])
    
    
    for i in range(1, iterations+1):
        random_index = ra.randint(0, len(training_data_set)-1) 
        input_wgts = wgts
        input_x = training_data_set[random_index][0]
        correct_answer = training_data_set[random_index][1]

        approx_answer = forward(w = input_wgts, x = input_x)
        wgts = backward(w = input_wgts, 
                           x = input_x,
                           correct_answer = correct_answer,
                           approx_answer = approx_answer,
                           alpha = alpha)
        df = df.append({'iteration': i, 
                        'input_wgts': input_wgts, 
                        'approx_answer': approx_answer, 
                        'correct_answer': correct_answer, 
                        'error': correct_answer - approx_answer, 
                        'new_wgts': wgts}, 
                       ignore_index=True)
    return(df)
        
        
def test(wgts, test_data_set):
    
    df = pd.DataFrame(columns=["iteration", "dataset", "approx_answer", "correct_answer", "error"])
    
    
    for i in range(0, len(test_data_set)):

        x = test_data_set[i][0]
        correct_answer = test_data_set[i][1]

        approx_answer = step(skalarprodukt(w = wgts, 
                                               x = x))

        df = df.append({'iteration': i, 
                        "dataset": x,
                        'approx_answer': approx_answer, 
                        'correct_answer': correct_answer, 
                        'error': correct_answer - approx_answer}, 
                       ignore_index=True)
    return(df)
        
        
        
# INPUTS
alpha = 1
iterations = 30
biasVal = 1
training_data_set = [(np.array([1,0,0]), 0), 
                     (np.array([1,0,1]), 1), 
                     (np.array([1,1,0]), 1), 
                     (np.array([1,1,1]), 1)]

wgts = np.zeros(len(training_data_set[0][0]))
wgts[0] = -biasVal

result_fit = fit(alpha = alpha, iterations = iterations, training_data_set = training_data_set, wgts = wgts)   
print("Result of Fitting: \n", result_fit, "\n\n")
    
optimized_wgts = result_fit["new_wgts"][result_fit.last_valid_index()]
print("Optimized Wgts are: ", optimized_wgts, "\n")

result_test = test(wgts = optimized_wgts, test_data_set = training_data_set)   
print("Result of Test: \n", result_test, "\n\n")
     
print("Plot Trainingsphase und Testphase:")
fig, axs = pyplot.subplots(2)
axs[0].plot(result_fit["iteration"], result_fit["error"], "g", linewidth=2)
axs[0].set_title("Training")
axs[0].set_ylabel("Error")
axs[0].set_xlabel("Iteration")
axs[1].plot(result_test["iteration"], result_test["error"], "g", linewidth=2)
axs[1].set_title("Test")
axs[1].set_ylabel("Error")
axs[1].set_xlabel("Test Row")
axs[1].set_xticks(range(len(training_data_set)))
pyplot.tight_layout()
pyplot.show()



print("Plots der Trennlinien:")
data_trennlinie = fit(alpha = alpha, iterations = iterations, training_data_set = training_data_set, wgts = np.array([-0.28, 0.02, 0.05]))   


pic_rows = 10
pic_cols = 3
fig, axs = pyplot.subplots(pic_rows, pic_cols, figsize=(9,40))

for k in range(pic_rows):
    for i in range(pic_cols):
         w = data_trennlinie["input_wgts"][k*pic_cols+i]
         x1 = np.arange(-0.5, 1.5, 0.01)
         x2 = np.zeros(len(np.arange(-0.5, 1.5, 0.01)))
         if w[2] != 0:
             x2 = - w[0] / w[2] - w[1] / w[2] * x1
         
         axs[k, i].plot(x1, x2, "g", linewidth=2)
         axs[k, i].set_ylim(-2, 6)
         axs[k, i].plot(0,0,"ko")
         axs[k, i].plot([0,1,1],[1,0,1],"ro", )
         axs[k, i].set_title("Schritt " + str(k*pic_cols+i))
#pyplot.tight_layout()
pyplot.show()


```
